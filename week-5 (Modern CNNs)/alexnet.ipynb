{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Assignment 2 - Modern CNNs (AlexNet)\n*This notebook was run by Arsyi Syarief Aziz (H071191003) for the Introduction to Deep Learning course (Unhas) taught by Dr. Risman Adnan.*","metadata":{"id":"kPUz7s7O0ezG"}},{"cell_type":"markdown","source":"This is the first part of this week's assigment. In this notebook, I will attempt to recreate AlexNet.","metadata":{"id":"1kNAVed00oG_"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nimport torch.optim as optim\nimport torch.nn.functional as F","metadata":{"id":"BCpzEwH657Mb","execution":{"iopub.status.busy":"2022-04-09T00:10:07.488987Z","iopub.execute_input":"2022-04-09T00:10:07.489410Z","iopub.status.idle":"2022-04-09T00:10:09.528562Z","shell.execute_reply.started":"2022-04-09T00:10:07.489267Z","shell.execute_reply":"2022-04-09T00:10:09.527389Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"cuda = torch.device('cuda')","metadata":{"id":"q3-CqlQ1KEEr","execution":{"iopub.status.busy":"2022-04-09T00:10:09.532932Z","iopub.execute_input":"2022-04-09T00:10:09.533399Z","iopub.status.idle":"2022-04-09T00:10:09.539645Z","shell.execute_reply.started":"2022-04-09T00:10:09.533318Z","shell.execute_reply":"2022-04-09T00:10:09.538316Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Importing Dataset","metadata":{"id":"bte5iPRGRDNQ"}},{"cell_type":"markdown","source":"I will use the MNIST dataset to train and test the model in this notebook. Because MNIST contains images of shape 28x28 pixels and because AlexNet accepts images of size 244x244 pixels, I will upscale each image in the dataset to 244x244 pixels.","metadata":{"id":"PrEHhDb5PkIe"}},{"cell_type":"code","source":"training_data = datasets.MNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=transforms.Compose([transforms.ToTensor(), transforms.Resize(224)])\n)\ntest_data = datasets.MNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=transforms.Compose([transforms.ToTensor(), transforms.Resize(224)]) # Upscale to 244x224 pixels\n)\n","metadata":{"id":"FhX60lnP8LLq","execution":{"iopub.status.busy":"2022-04-09T00:10:09.541692Z","iopub.execute_input":"2022-04-09T00:10:09.542394Z","iopub.status.idle":"2022-04-09T00:10:11.434078Z","shell.execute_reply.started":"2022-04-09T00:10:09.542297Z","shell.execute_reply":"2022-04-09T00:10:11.433027Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n                 dataset=training_data,\n                 batch_size=512,\n                 shuffle=True,\n                 num_workers=2,\n                 pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(\n                dataset=test_data,\n                batch_size=512,\n                shuffle=False,\n                num_workers=2,\n                pin_memory=True)","metadata":{"id":"JvCVDzeP3ZMl","execution":{"iopub.status.busy":"2022-04-09T00:10:11.437097Z","iopub.execute_input":"2022-04-09T00:10:11.437737Z","iopub.status.idle":"2022-04-09T00:10:11.445628Z","shell.execute_reply.started":"2022-04-09T00:10:11.437687Z","shell.execute_reply":"2022-04-09T00:10:11.444497Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Viewing samples of the dataset","metadata":{"id":"jcrx_r_XRoQD"}},{"cell_type":"code","source":"figure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(label)\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()","metadata":{"id":"t1Me2hEdDRL3","outputId":"e8b2b870-0533-4bc5-830a-1b0741819777","execution":{"iopub.status.busy":"2022-04-09T00:10:11.447946Z","iopub.execute_input":"2022-04-09T00:10:11.448182Z","iopub.status.idle":"2022-04-09T00:10:12.040266Z","shell.execute_reply.started":"2022-04-09T00:10:11.448138Z","shell.execute_reply":"2022-04-09T00:10:12.039368Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Definining Helper Functions","metadata":{"id":"aTtluDE6mIqc"}},{"cell_type":"markdown","source":"This function will be used to train a neural network","metadata":{"id":"-CKbG0S0mQKG"}},{"cell_type":"code","source":"def train(net, train_loader, epochs=10, lr=0.001, momentum=0.9, weight_decay=0.0005): \n\n    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n    criterion = nn.CrossEntropyLoss()\n\n    net.to(cuda)\n    \n    for epoch in range(epochs):  # loop over the dataset multiple times\n        running_loss = 0.0\n\n        net.train()\n        for X, y in train_loader:\n            X, y = X.to(cuda), y.to(cuda)\n\n            # zero the parameter gradients\n            optimizer.zero_grad(set_to_none=True)\n\n            # forward + backward + optimize\n            outputs = net(X)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        \n        print(f'Epoch: {epoch + 1}, loss: {running_loss / len(train_loader.dataset):.6f}')\n\n    print('Finished Training')\n    total, correct = test(net, train_loader)\n    print(f'Train accuracy: {(correct/total)*100}%')","metadata":{"id":"bHmZBkYymNJz","execution":{"iopub.status.busy":"2022-04-09T00:10:12.042951Z","iopub.execute_input":"2022-04-09T00:10:12.043294Z","iopub.status.idle":"2022-04-09T00:10:12.053504Z","shell.execute_reply.started":"2022-04-09T00:10:12.043237Z","shell.execute_reply":"2022-04-09T00:10:12.051984Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"This function will be used to visualize a neural network's prediction on n-samples of a dataset and calculate its overall accuracy.","metadata":{"id":"spLBgSm4ma_w"}},{"cell_type":"code","source":"def test(net, test_loader):\n    \n    total = 0\n    correct = 0\n\n    for X, y in test_loader:\n        net.eval()\n        X, y = X.to(cuda), y.to(cuda)\n\n        y_hat = net(X).argmax(axis=1)\n        total += X.shape[0]\n        correct += (y_hat == y).sum()\n\n    return total, correct\n  ","metadata":{"id":"Dgw-cVoMm1P8","execution":{"iopub.status.busy":"2022-04-09T00:10:12.055484Z","iopub.execute_input":"2022-04-09T00:10:12.056127Z","iopub.status.idle":"2022-04-09T00:10:12.068243Z","shell.execute_reply.started":"2022-04-09T00:10:12.056079Z","shell.execute_reply":"2022-04-09T00:10:12.067300Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## AlexNet","metadata":{"id":"yagl3aDA3Wiz"}},{"cell_type":"markdown","source":"The first architecture that I will implement is AlexNet. This architecture is similar to LeNet, but has more convolutional layers; consisting of five convolutional layers and three fully connected layers.\n\nThe first convolution layer filters a 244x244 pixel image against 96 kernels of size 11x11 with a stride of 4 pixels. The second convolutional layer takes the output of the previous layer and filters it against 256 kernels of size 5x5 and max pools it against a kernel of size 3x3. Unlike the first and second convolutional layers, the third and fourth layers don't have any max pooling layers. The third convolutional layer filters the previous layer's output against 384 kernels of size 3x3, and the fourth convolutional layer filters the third layer's output against 384 kernels of size 3x3. Continuing on, the output of the fourth layer is inputted into the fifth convolutional layer with 256 kernels of size 3x3, which is then max pooled with a kernel of size 3x3. Finally, the output of the previous layer is inputted into three fully connected layers, each containing 4096 neurons.","metadata":{"id":"VxXrSLrzjTM8"}},{"cell_type":"code","source":"alex_net = nn.Sequential(\n    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nn.Flatten(),\n    nn.Linear(6400, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(4096, 4096), nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(4096, 10)\n)","metadata":{"id":"TIcEMc2-DGht","execution":{"iopub.status.busy":"2022-04-09T00:10:12.069984Z","iopub.execute_input":"2022-04-09T00:10:12.070609Z","iopub.status.idle":"2022-04-09T00:10:12.504440Z","shell.execute_reply.started":"2022-04-09T00:10:12.070564Z","shell.execute_reply":"2022-04-09T00:10:12.503463Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train(alex_net, train_loader, lr=0.001, epochs=10) ","metadata":{"id":"ufhelQkTInz8","outputId":"3bff4799-1487-4565-e52b-d1f36a123af6","execution":{"iopub.status.busy":"2022-04-09T00:10:12.506628Z","iopub.execute_input":"2022-04-09T00:10:12.507140Z","iopub.status.idle":"2022-04-09T00:25:13.706336Z","shell.execute_reply.started":"2022-04-09T00:10:12.507097Z","shell.execute_reply":"2022-04-09T00:25:13.705115Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"total, correct = test(alex_net, test_loader)  ","metadata":{"id":"C48URL348-eH","execution":{"iopub.status.busy":"2022-04-09T00:25:13.710590Z","iopub.execute_input":"2022-04-09T00:25:13.710877Z","iopub.status.idle":"2022-04-09T00:25:26.710924Z","shell.execute_reply.started":"2022-04-09T00:25:13.710843Z","shell.execute_reply":"2022-04-09T00:25:26.709752Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(f\"Test Accuracy: {(correct/total)*100}% ({correct}/{total})\")","metadata":{"id":"v8igdpIIRVtn","execution":{"iopub.status.busy":"2022-04-09T00:25:26.713134Z","iopub.execute_input":"2022-04-09T00:25:26.713514Z","iopub.status.idle":"2022-04-09T00:25:26.720496Z","shell.execute_reply.started":"2022-04-09T00:25:26.713471Z","shell.execute_reply":"2022-04-09T00:25:26.719403Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def viz_results(net, dataset, num_samples=5):\n    net.cpu()\n    for i, (x, y) in enumerate(iter(dataset), 1):\n        if i <= num_samples:\n            \n            x, y = x, y\n            x = x.reshape(1, 1, x.shape[1], x.shape[2])\n\n            y_hat = net(x).argmax()\n\n            plt.imshow(x.reshape(x.shape[1], x.shape[2], -1).squeeze())\n            plt.title(f'{i}. Actual: {y}; Predicted: {y_hat}')\n            plt.show()\n        else:\n            break","metadata":{"execution":{"iopub.status.busy":"2022-04-09T00:28:36.717486Z","iopub.execute_input":"2022-04-09T00:28:36.717780Z","iopub.status.idle":"2022-04-09T00:28:36.725486Z","shell.execute_reply.started":"2022-04-09T00:28:36.717748Z","shell.execute_reply":"2022-04-09T00:28:36.724388Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"viz_results(alex_net, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T00:28:37.355882Z","iopub.execute_input":"2022-04-09T00:28:37.356317Z","iopub.status.idle":"2022-04-09T00:28:39.548264Z","shell.execute_reply.started":"2022-04-09T00:28:37.356268Z","shell.execute_reply":"2022-04-09T00:28:39.547346Z"},"trusted":true},"execution_count":21,"outputs":[]}]}